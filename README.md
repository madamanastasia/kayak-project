# ğŸ§³ Planifiez votre voyage â€“ Projet Data Engineering

## ğŸ“Œ PrÃ©sentation

Ce projet met en place un pipeline de donnÃ©es permettant de collecter et dâ€™analyser des informations touristiques pour plusieurs villes franÃ§aises.

Lâ€™objectif est de dÃ©montrer des compÃ©tences pratiques en :

- Web scraping  
- IntÃ©gration dâ€™API  
- Structuration de donnÃ©es  
- Analyse avec SQL  
- Visualisation  

Lâ€™ensemble du projet est organisÃ© de maniÃ¨re claire afin de distinguer la phase de collecte, la phase de stockage des donnÃ©es et la phase dâ€™analyse.

---

# ğŸ›  Technologies utilisÃ©es

- Python  
- requests  
- pandas  
- BeautifulSoup  
- SQLite / SQL  
- Jupyter Notebook  

---

# ğŸ“‚ Structure du repository

```
kayak_project/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ weather.ipynb
â”‚   â”œâ”€â”€ hotels.ipynb
â”‚   â””â”€â”€ sql_analysis.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cities.csv
â”‚   â”œâ”€â”€ weather_daily.csv
â”‚   â””â”€â”€ hotels.csv
â”œâ”€â”€ database/
â”‚   â””â”€â”€ kayak.db
â””â”€â”€ README.md
```

---

# ğŸ§© Description des fichiers

## ğŸ““ weather.ipynb

Notebook dÃ©diÃ© Ã  la collecte des donnÃ©es mÃ©tÃ©orologiques :

- Appels API pour rÃ©cupÃ©rer les donnÃ©es mÃ©tÃ©o
- Structuration des rÃ©ponses JSON
- Nettoyage des donnÃ©es
- Export vers `weather_daily.csv`

Ce notebook montre lâ€™intÃ©gration dâ€™une API REST et la transformation des donnÃ©es reÃ§ues.

---

## ğŸ““ hotels.ipynb

Notebook consacrÃ© au web scraping :

- Extraction des informations hÃ´tels
- Parsing du HTML
- Nettoyage et structuration
- Export vers `hotels.csv`

Ce notebook dÃ©montre la capacitÃ© Ã  collecter des donnÃ©es non structurÃ©es et Ã  les transformer en donnÃ©es exploitables.

---

## ğŸ““ sql_analysis.ipynb

Notebook dâ€™analyse SQL :

- CrÃ©ation des tables relationnelles
- Import des fichiers CSV dans la base
- Jointures entre villes, mÃ©tÃ©o et hÃ´tels
- RequÃªtes analytiques (agrÃ©gations, filtres, tri)
- GÃ©nÃ©ration de visualisations

Il illustre lâ€™utilisation du SQL pour exploiter les donnÃ©es collectÃ©es.

---

## ğŸ“„ cities.csv

Contient la liste des villes Ã©tudiÃ©es ainsi que leurs coordonnÃ©es GPS.

---

## ğŸ“„ weather_daily.csv

DonnÃ©es mÃ©tÃ©orologiques collectÃ©es via API, structurÃ©es et nettoyÃ©es.

---

## ğŸ“„ hotels.csv

DonnÃ©es issues du scraping :

- Nom  
- Prix  
- Note  
- Description  
- Localisation  

---

## ğŸ—„ kayak.db

Base de donnÃ©es SQLite locale utilisÃ©e pour :

- Stocker les tables relationnelles  
- ExÃ©cuter les requÃªtes SQL  
- Centraliser les donnÃ©es avant analyse  

Cette base permet de simuler un environnement de base de donnÃ©es relationnelle dans un contexte local.

---

# ğŸ Conclusion

Ce projet dÃ©montre la capacitÃ© Ã  collecter, structurer et analyser des donnÃ©es Ã  travers un workflow complet, en utilisant Python et SQL dans un environnement reproductible.

---

 # ğŸ‘¤ Auteur

Anastasiia Belosludtseva
